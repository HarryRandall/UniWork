### Types of Testing
- **Functional Testing:** Validates software functionality against requirements (e.g., unit, feature and system testing).
-  **Performance Testing:** Checks responsiveness, stability, and scalability under load.
- **Security Testing:** Identifies vulnerabilities, ensuring data protection.
- **Regression Testing:** Confirms that recent changes haven't introduced new bugs.
- **Boundary-Value Testing:** Tests edge cases to catch off-by-one and other boundary-related errors.
### Test Automation
- Automates repetitive testing, ensuring consistency and saving time, especially in regression testing.
- **Unit Tests:** Fast, cover individual functions or methods.
- **Integration Tests:** Ensure multiple components interact correctly.
- **System Tests:** Full end-to-end tests simulating real user interactions.
- **Continuous Integration (CI):** Automatically runs tests with each code change.
### Limitations of Testing
- **Coverage:** Achieving 100% coverage doesn't ensure complete reliability.
- **Oracle Problem:** Difficult to define expected outcomes for all possible scenarios.
- **Cost and Time:** Testing large systems thoroughly can be resource-intensive.
- Testing can't prove the absence of bugs, only detect their presence.
### Fuzzing
- **Fuzz Testing:** Inputs random data to detect unexpected behaviours and crashes.
- Common for security testing, especially in low-level languages prone to memory issues.
- **Strengths:** Simple and effective for finding edge-case bugs.
- **Limitations:** May miss subtle bugs and requires handling of nonsensical inputs.

### Performance Testing
- Evaluates the system's speed, resource usage, and scalability.
- **Stress Testing:** Puts system under extreme load to find breaking points.
- **Soak Testing:** Tests over extended periods to detect issues like memory leaks. 
- Tools: JMeter, profiling tools, and real-time monitoring for identifying bottlenecks.
### Chaos Engineering
- **Chaos Testing:** Deliberately induces failure to test system resilience (e.g., Netflix's Chaos Monkey).
- **Goals:** Expose weak points, validate redundancy, and prepare for disaster recovery.
- Tests involved simulated network failures, server crashes, and other disruptions.
### A/B Testing
- Compares two versions (A and B) of a feature to measure user response.
- Used for UX improvements and feature validate by deploying different versions of subsets of users.
- Metrics from A/B tests can inform product decisions based on real user interactions.